{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57130cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.0\n",
      "Numpy version: 1.20.3\n",
      "Pytorch version: 1.10.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 714d00dffe6653e21260160666c4c201ab66511b\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.7\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 8.4.0\n",
      "Tensorboard version: 2.7.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.11.1\n",
      "tqdm version: 4.62.3\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pandas version: 1.3.4\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from os import makedirs, path, getcwd\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_dataset, num_batches_per_epoch\n",
    "from options import args_parser\n",
    "from update import test_inference\n",
    "from models import get_model, get_optimizer\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from update import ClientShard, test_inference\n",
    "\n",
    "from tqdm import tqdm\n",
    "from os import path, getcwd, makedirs\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "sys.path.insert(0, path.join(getcwd(), \"..\", \"..\"))\n",
    "\n",
    "from options import args_parser\n",
    "#from update import ClientShard, test_inference\n",
    "from models import get_model, VGG, CNNMnist\n",
    "from utils import (\n",
    "    get_dataset,\n",
    "    average_weights,\n",
    "    exp_details,\n",
    ")\n",
    "\n",
    "#MONAI Imports\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch,ArrayDataset, GridPatchDataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    AsChannelFirst,\n",
    "    Compose,\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    LabelToMask,\n",
    "    LoadImage,\n",
    "    MapTransform,\n",
    "    NormalizeIntensity,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandSpatialCrop,\n",
    "    Resize,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    Spacing,\n",
    "    Spacingd,\n",
    "    ScaleIntensity, \n",
    "    SpatialCrop,\n",
    "    ToTensor,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Resized\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "\n",
    "from glob import glob\n",
    "import os.path\n",
    "import nibabel as nib\n",
    "from nibabel import load, save, Nifti1Image\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de17ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser('FL experiments parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bece4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='random seed', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# federated arguments (Notation for the arguments followed from paper)\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                    help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=100,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--test_frac', type=float, default=0.1,\n",
    "                    help='the fraction of data to go into test split.')\n",
    "parser.add_argument('--frac', type=float, default=0.1,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=10,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "parser.add_argument('--local_bs', type=int, default=8,\n",
    "                    help=\"local batch size: B\")\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.5,\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "\n",
    "# model arguments\n",
    "parser.add_argument('--task', type=str, default='cv', help='task name')\n",
    "parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "parser.add_argument('--kernel_num', type=int, default=9,\n",
    "                    help='number of each kind of kernel')\n",
    "parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
    "                    help='comma-separated kernel size to \\\n",
    "                    use for convolution')\n",
    "parser.add_argument('--num_channels', type=int, default=3, help=\"number \\\n",
    "                    of channels of imgs\")\n",
    "parser.add_argument('--norm', type=str, default='batch_norm',\n",
    "                    help=\"batch_norm, layer_norm, or None\")\n",
    "parser.add_argument('--num_filters', type=int, default=32,\n",
    "                    help=\"number of filters for conv nets -- 32 for \\\n",
    "                    mini-imagenet, 64 for omiglot.\")\n",
    "parser.add_argument('--max_pool', type=str, default='True',\n",
    "                    help=\"Whether use max pooling rather than \\\n",
    "                    strided convolutions\")\n",
    "\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='cifar', help=\"name \\\n",
    "                    of dataset\")\n",
    "parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
    "                    of classes\")\n",
    "parser.add_argument('--gpu', default=None, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "parser.add_argument('--optimizer', type=str, default='sgd', help=\"type \\\n",
    "                    of optimizer\")\n",
    "parser.add_argument('--iid', type=int, default=1,\n",
    "                    help='Default set to IID. Set to 0 for non-IID.')\n",
    "parser.add_argument('--unequal', type=int, default=0,\n",
    "                    help='whether to use unequal data splits for  \\\n",
    "                    non-i.i.d setting (use 0 for equal splits)')\n",
    "parser.add_argument('--stopping_rounds', type=int, default=10,\n",
    "                    help='rounds of early stopping')\n",
    "parser.add_argument('--ROOT_DATA', type=str, default='./data/',\n",
    "                    help='Root data dyrectory ')\n",
    "\n",
    "parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edee9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIEU = 'sebastian_insel'\n",
    "\n",
    "if LIEU=='sebastian_insel':\n",
    "    root_exp  = '/home/sebastian/experiments/fedem/'\n",
    "    root_data = '/media/sebastian/data/ASAP/BRATS_2019_ubelix/center_wise/'\n",
    "    brats_nifti__dir_paths = natsorted(glob(root_data+\"**/**/\"))\n",
    "\n",
    "if LIEU=='sebastian_laptop':\n",
    "    root_exp  = '/Users/sebastianotalora/work/postdoc/federated_learning/fedem/'\n",
    "    root_data = '/Users/sebastianotalora/work/postdoc/federated_learning/data/'\n",
    "    brats_nifti__dir_paths = natsorted(glob(root_data+'brats/'\"**/**/\"))\n",
    "len(brats_nifti__dir_paths)#Should be 259\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334d96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([\"--task\", \"cv\", \"--dataset\", \"brats\", \"--epochs\", \"2\", \"--num_users\", \"4\", \n",
    "                          \"--ROOT_DATA\", root_data, \"--gpu\", \"True\",\n",
    "                          \"--frac\", \"2\", \"--local_ep\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b1b656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/sebastian/data/ASAP/BRATS_2019_ubelix/center_wise/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ROOT_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694e95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  'cuda' if args.gpu else 'cpu'\n",
    "path_project = path.abspath('..')\n",
    "logger = SummaryWriter('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e85a959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8c9a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 2, 'num_users': 4, 'test_frac': 0.1, 'frac': 2.0, 'local_ep': 5, 'local_bs': 8, 'lr': 0.01, 'momentum': 0.5, 'task': 'cv', 'model': 'cnn', 'kernel_num': 9, 'kernel_sizes': '3,4,5', 'num_channels': 3, 'norm': 'batch_norm', 'num_filters': 32, 'max_pool': 'True', 'dataset': 'brats', 'num_classes': 10, 'gpu': 'True', 'optimizer': 'sgd', 'iid': 1, 'unequal': 0, 'stopping_rounds': 10, 'ROOT_DATA': '/media/sebastian/data/ASAP/BRATS_2019_ubelix/center_wise/', 'verbose': 1, 'seed': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f189b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "loading BraTS data from /media/sebastian/data/ASAP/BRATS_2019_ubelix/center_wise//\n",
      "180 28 51\n",
      "Loading BraTS segmentation Network\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_dataset, test_dataset, user_groups, user_groups_test = get_dataset(args)\n",
    "\n",
    "# Load monai segmentation model.\n",
    "global_model = get_model(args=args, img_size=train_dataset[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ac3f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegResNet(\n",
       "  (act): ReLU(inplace=True)\n",
       "  (convInit): Convolution(\n",
       "    (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  )\n",
       "  (down_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_samples): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode=trilinear)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode=trilinear)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode=trilinear)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_final): Sequential(\n",
       "    (0): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Convolution(\n",
       "      (conv): Conv3d(16, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout3d(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0274fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Task      : cv\n",
      "    Model     : cnn\n",
      "    Optimizer : sgd\n",
      "    Learning  : 0.01\n",
      "    Global Rounds   : 2\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    Fraction of users  : 2.0\n",
      "    Local Batch size   : 8\n",
      "    Local Epochs       : 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Start of Federated learning. {{{\n",
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "#print(global_model)\n",
    "# copy weights\n",
    "global_weights = global_model.state_dict()\n",
    "# Training\n",
    "train_loss, train_dice = [], []\n",
    "print_every = 1\n",
    "exp_details(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0386c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_bs = 1\n",
    "args.epochs   = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1396a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_users = np.array([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb9ce26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f22cb3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BraTS segmentation Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/envs/fed/lib/python3.9/site-packages/monai/transforms/post/array.py:182: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.\n",
      "  warnings.warn(\"`threshold_values=True/False` is deprecated, please use `threshold=value` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 0 | Hidden client num : 0 | Local Epoch : 4 | [10/12 (83%)]\tLoss: 0.928311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/envs/fed/lib/python3.9/site-packages/monai/transforms/post/array.py:182: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.\n",
      "  warnings.warn(\"`threshold_values=True/False` is deprecated, please use `threshold=value` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 0 | Hidden client num : 1 | Local Epoch : 4 | [80/83 (96%)]\tLoss: 0.972865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/envs/fed/lib/python3.9/site-packages/monai/transforms/post/array.py:182: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.\n",
      "  warnings.warn(\"`threshold_values=True/False` is deprecated, please use `threshold=value` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 0 | Hidden client num : 2 | Local Epoch : 4 | [60/65 (92%)]\tLoss: 0.958628"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/envs/fed/lib/python3.9/site-packages/monai/transforms/post/array.py:182: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.\n",
      "  warnings.warn(\"`threshold_values=True/False` is deprecated, please use `threshold=value` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 0 | Hidden client num : 3 | Local Epoch : 4 | [0/4 (0%)]\tLoss: 0.935884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Hidden client num::   0%|                                                                                                                                                       | 0/4 [00:00<?, ?it/s]/home/sebastian/anaconda3/envs/fed/lib/python3.9/site-packages/monai/transforms/post/array.py:182: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.\n",
      "  warnings.warn(\"`threshold_values=True/False` is deprecated, please use `threshold=value` instead.\")\n",
      "Evaluating: Hidden client num::   0%|                                                                                                                                                       | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (903168) must match the size of tensor b (72) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_799473/179999226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                    \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                    device=device)\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_shard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mlist_dice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdice_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/experiments/fedem/update.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                     \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (903168) must match the size of tensor b (72) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Load monai segmentation model.\n",
    "global_model = get_model(args=args, img_size=train_dataset[0][0].shape)\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    local_weights, local_losses = [], []\n",
    "    global_model.train()\n",
    "    # Randomly sample a fraction of clients and retrieve their ids.\n",
    "    user_frac = 2#max(int(args.frac * args.num_users), 1)\n",
    "    #idxs_users = np.random.choice(range(args.num_users), user_frac, replace=False)\n",
    "    idxs_users = np.array([0,1,2,3]) # In the BraTS case we train with ALL the clients\n",
    "    \n",
    "    for hidden_client_idx, idx in enumerate(idxs_users):\n",
    "        client_shard = ClientShard(args=args,\n",
    "                                   client_idx=hidden_client_idx,\n",
    "                                   dataset=train_dataset,\n",
    "                                   idxs=user_groups[idx],\n",
    "                                   logger=logger,\n",
    "                                   device=device)\n",
    "        updated_local_model, loss = client_shard.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(updated_local_model))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "        \n",
    "    # Calculate averaged model weights from all the client trained models.\n",
    "    global_weights = average_weights(local_weights)\n",
    "    # Update global weights with the averaged model weights.\n",
    "    global_model.load_state_dict(global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "    \n",
    "    # Calculate avg training DICE over all users at every epoch\n",
    "    list_dice, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    # Remove trained clients from heldout evaluation.\n",
    "    heldout_clients = list(range(args.num_users))#We evaluate in the test set of each client\n",
    "\n",
    "    for heldout_client_idx, idx in tqdm(enumerate(heldout_clients),\n",
    "                                        desc='Evaluating: Hidden client num:',\n",
    "                                        total=len(heldout_clients)):\n",
    "        client_shard = ClientShard(args=args,\n",
    "                                   client_idx=heldout_client_idx,\n",
    "                                   dataset=train_dataset,\n",
    "                                   idxs=user_groups[idx],\n",
    "                                   logger=logger,\n",
    "                                   device=device)\n",
    "        loss, dice_client = client_shard.inference(model=global_model)\n",
    "        list_dice.append(dice_client)\n",
    "        list_loss.append(loss)\n",
    "    train_dice.append(sum(list_dice) / len(list_dice))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train DICE: {:.2f}% \\n'.format(100 * train_dice[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2424b706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in client_shard.testloader:\n",
    "    print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3367aa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: Hidden client num:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 6227.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for heldout_client_idx, idx in tqdm(enumerate(heldout_clients),\n",
    "                                        desc='Evaluating: Hidden client num:',\n",
    "                                        total=len(heldout_clients)):\n",
    "    print(heldout_client_idx, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference after completion of training\n",
    "test_dice, test_loss = test_inference(args=args, model=global_model, test_dataset=test_dataset, device=device)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train DICE: {:.2f}%\".format(100*train_dice[-1]))\n",
    "print(\"|---- Test DICE: {:.2f}%\".format(100*test_dice))\n",
    "### }}} End of Federated learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a0be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
